{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Pre-process the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check how many observations per patient there are**\n",
    "Just to see what amount of data we're dealing with we want to check how many observations per each patient there are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2014-06-09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  size\n",
       "112  2014-06-09     1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/dataset_mood_smartphone.csv\")\n",
    "#counted = df.groupby([\"time\"]).size()\n",
    "df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "#df['time'].dtype\n",
    "counted = df.groupby([df['time'].dt.date]).size()\n",
    "print (len(counted))\n",
    "counted = counted.to_frame().reset_index()\n",
    "counted.columns = ['date', 'size']\n",
    "\n",
    "#print (counted.columns)\n",
    "##counted.head()\n",
    "#print (counted)\n",
    "counted[counted['size']==counted['size'].min()]\n",
    "#hist = counted.hist(bins=113)\n",
    "#plt.figure(figsize=(9,7), dpi=100)\n",
    "#plt.hist(counted)\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're checking for the amount of observations per each vatiable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "variable\n",
      "activity                22965\n",
      "appCat.builtin          91288\n",
      "appCat.communication    74276\n",
      "appCat.entertainment    27125\n",
      "appCat.finance            939\n",
      "appCat.game               813\n",
      "appCat.office            5642\n",
      "appCat.other             7650\n",
      "appCat.social           19145\n",
      "appCat.travel            2846\n",
      "appCat.unknown            939\n",
      "appCat.utilities         2487\n",
      "appCat.weather            255\n",
      "call                     5239\n",
      "circumplex.arousal       5643\n",
      "circumplex.valence       5643\n",
      "mood                     5641\n",
      "screen                  96578\n",
      "sms                      1798\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counted = df.groupby([\"variable\"]).size()\n",
    "print (len(counted))\n",
    "print (counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are __***27 patients***__, names from 01 to 33 (some numbers are missing)\n",
    "\n",
    "There are __***19 variables***__\n",
    "\n",
    "\n",
    "\n",
    "**Making individual files for each patient:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawFile = \"dataset_mood_smartphone.csv\"\n",
    "\n",
    "with open(RawFile) as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    #input_header = reader.next()\n",
    "    for row in reader:\n",
    "        patient = row[1]\n",
    "        CleanedFile = \"./data/\" + patient +\".csv\"\n",
    "        with open(CleanedFile, 'a', newline='') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            if row[4] != \"NA\": #deleting rows with NA as VALUE\n",
    "                writer.writerow(row[2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acording to the course desctiption, each variable will be dealt with as shown below to get a day summary for each patient.\n",
    "\n",
    "activity                Numerical value - use mean\n",
    "\n",
    "appCat.builtin          Time - add times\n",
    "\n",
    "appCat.communication    Time - add times\n",
    "\n",
    "appCat.entertainment    Time - add times\n",
    "\n",
    "appCat.finance          Time - add times\n",
    "\n",
    "appCat.game             Time - add times\n",
    "\n",
    "appCat.office           Time - add times\n",
    "\n",
    "appCat.other            Time - add times\n",
    "\n",
    "appCat.social           Time - add times\n",
    "\n",
    "appCat.travel           Time - add times\n",
    "\n",
    "appCat.unknown          Time - add times\n",
    "\n",
    "appCat.utilities        Time - add times\n",
    "\n",
    "appCat.weather          Time - add times\n",
    "\n",
    "call                    Numerical value - add\n",
    "\n",
    "circumplex.arousal      Numerical value - use mean\n",
    "\n",
    "circumplex.valence      Numerical value - use mean\n",
    "\n",
    "mood                    Numerical value - use mean\n",
    "\n",
    "screen                  Time\n",
    "\n",
    "SMS                     Numerical values - add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/AS14.05.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-efe7032bef46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/AS14.05.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcounted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcounted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\marta\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\marta\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\marta\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\marta\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\marta\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./data/AS14.05.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/AS14.05.csv\")\n",
    "counted = df.groupby([\"variable\"]).size()\n",
    "print (len(counted))\n",
    "print (counted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time         object\n",
       "variable     object\n",
       "value       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Want to creat a table/csv that would summarize each day. Then we can proceed with another algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a day by day summary of the data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables= ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "patient = ['01', '02','03','05','06','07','08','09','12','13','14','15','16','17','19','20','23','24','25','26','27','28','29','30','31','32','33']\n",
    "for patientnumber in patient:\n",
    "    inpath = \"./data/AS14.\"+patientnumber+\".csv\"\n",
    "    outpath = \"./data/summary/AS14.\" + patientnumber + \".csv\"\n",
    "    df = pd.read_csv(inpath)\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"]) \n",
    "    #df['date_minus_time'] = df[\"time\"].apply( lambda df : datetime.datetime(year=df.year, month=df.month, day=df.day))\n",
    "    #df.set_index(df[\"date_minus_time\"],inplace=True) # using date as index \n",
    "    df.index = df['time']\n",
    "    df = df.sort_index()\n",
    "\n",
    "\n",
    "    byday = [] #variables summmed/meaned for each day\n",
    "    for i, j in enumerate(variables):\n",
    "        #print(i, j)\n",
    "        dfvariables = df.loc[df['variable'] == j]\n",
    "        if i < 4:\n",
    "            data = dfvariables['value'].resample('D').mean()\n",
    "    #         print(i, j, len(data))\n",
    "    #         print (len(data))\n",
    "        else:\n",
    "            data = dfvariables['value'].resample('D').sum()\n",
    "            #print(i, j, len(data))\n",
    "            #print (len(data))\n",
    "        byday.append(data)\n",
    "\n",
    "    daysSet = pd.concat(byday, axis=1)\n",
    "    daysSet.columns = variables\n",
    "    daysSet= daysSet.replace(0, np.nan)\n",
    "\n",
    "    #print(daysSet)\n",
    "    daysSet.to_csv(outpath, encoding='utf-8', index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# variables= ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "# patient = ['01', '02','03','05','06','07','08','09','12','13','14','15','16','17','19','20','23','24','25','26','27','28','29','30','31','32','33']\n",
    "\n",
    "# for patientnumber in patient:\n",
    "#     inpath = \"./data/summary/AS14.\" + patientnumber + \".csv\"\n",
    "#     outstats = \"./data/stats/\" + patientnumber + \".txt\"\n",
    "   \n",
    "#     df = pd.read_csv(inpath)\n",
    "#     with open(outstats, 'a', newline='') as statsFile:\n",
    "        \n",
    "#         for i in variables:\n",
    "#             j = df[i]\n",
    "            \n",
    "#             nanSum = j.isna().sum()\n",
    "#             legitVal = len(j) - nanSum\n",
    "#             statsFile.write(\"There are \" + str(legitVal) + \" instances of \" + i + \"\\n\")\n",
    "#             #array.append(legitVal)\n",
    "#             #statsFile.write(\". For each day it is \\n\")\n",
    "#             #statsFile.write(str(j) + \"\\n\")\n",
    "#             del j\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that some of the data variables only appear for few days so we want to check the frequency of each variable of each patient to maybe be able to delete some columns (variables) with very few values.\n",
    "\n",
    "So here we're are creating the frequency summary for each patient for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables= ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "patient = ['01', '02','03','05','06','07','08','09','12','13','14','15','16','17','19','20','23','24','25','26','27','28','29','30','31','32','33']\n",
    "matrixForAll = ['patientNumber', 'mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "outstats = \"./data/stats/AllStatsFrequency.csv\"\n",
    "with open (outstats,'a', newline='') as statsFile:\n",
    "\n",
    "    writer = csv.writer(statsFile)\n",
    "    writer.writerow(matrixForAll)\n",
    "\n",
    "    for patientnumber in patient:\n",
    "        inpath = \"./data/summary/\" + patientnumber + \"_compressed.csv\"\n",
    "        df = pd.read_csv(inpath)\n",
    "        array = []\n",
    "        array.append(patientnumber)\n",
    "\n",
    "        for i in variables:\n",
    "            j = df[i]\n",
    "\n",
    "            nanSum = j.isna().sum()\n",
    "            legitVal = len(j) - nanSum\n",
    "            array.append(legitVal)\n",
    "            del j\n",
    "            \n",
    "        writer.writerow(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to explore some statistics aboout the data so we create a sum/mean values that correspont to each patient for each variable and put it in a csv for further analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9684d76cf51d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#         statsFile.write(item + \",\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     statsFile.write(\"\\n\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatsFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrixForAll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "variables= ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "patient = ['01', '02','03','05','06','07','08','09','12','13','14','15','16','17','19','20','23','24','25','26','27','28','29','30','31','32','33']\n",
    "matrixForAll = ['patientNumber', 'mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "outstats = \"./data/stats/AllStats.csv\"\n",
    "with open (outstats,'a', newline='') as statsFile:\n",
    "\n",
    "    writer = csv.writer(statsFile)\n",
    "    writer.writerow(matrixForAll)\n",
    "\n",
    "    for patientnumber in patient:\n",
    "        inpath = \"./data/summary/AS14.\" + patientnumber + \".csv\"\n",
    "        df = pd.read_csv(inpath)\n",
    "        array = []\n",
    "        array.append(patientnumber)\n",
    "\n",
    "        for k, i in enumerate(variables):\n",
    "            j = df[i]\n",
    "            mean = j.mean()\n",
    "            if k < 4:\n",
    "                data = j.mean()\n",
    "    #         print(i, j, len(data))\n",
    "    #         print (len(data))\n",
    "            else:\n",
    "                data = j.sum()\n",
    "            array.append(data)\n",
    "            del j\n",
    "            \n",
    "        writer.writerow(array)\n",
    "    #print(matrixForAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'load_iris'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d72778bcefb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# load the iris dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0miris\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# separate the data from the target attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\marta\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'load_iris'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize the data attributes for the Iris dataset.\n",
    "#from sklearn.datasets import load_iris\n",
    "from sklearn import preprocessing\n",
    "patient = ['01', '02','03','05','06','07','08','09','12','13','14','15','16','17','19','20','23','24','25','26','27','28','29','30','31','32','33']\n",
    "for patientnumber in patient:\n",
    "    inpath = \"./data/individualPatientData/AS14.\"+patientnumber+\".csv\"\n",
    "    df = pd.read_csv(inpath)\n",
    "    # load the iris dataset\n",
    "    iris = df.load_iris()\n",
    "    print(iris.data.shape)\n",
    "    # separate the data from the target attributes\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    # normalize the data attributes\n",
    "    normalized_X = preprocessing.normalize(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we try to transform the data in such way that it can be all be valuable. \n",
    "\n",
    "## Delete Unnecessary Rows\n",
    "## Perform One-Hot-Encoding for day of week (day_0 = monday, and so on...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables= ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "patient = ['01', '02','03','05','06','07','08','09','12','13','14','15','16','17','19','20','23','24','25','26','27','28','29','30','31','32','33']\n",
    "for patientnumber in patient:\n",
    "    inpath = \"./data/summary/AS14.\" + patientnumber + \".csv\"\n",
    "    outpath = \"./data/summary/\" + patientnumber + \"_compressed.csv\"\n",
    "    df = pd.read_csv(inpath)\n",
    "\n",
    "    #df.dtypes\n",
    "    #print (df) #cheched datatype of each column\n",
    "\n",
    "    #df = df.dropna(subset=[\"mood\"], inplace=True) #drops the rows with empty mood\n",
    "\n",
    "    df.dropna(thresh=4, inplace=True)     # drop the rows with at least 4 LEGIT values \n",
    "                                                # GOOD TO START WITH CAUSE THERE IS NO POINT IN FILLING MOOD DATA \n",
    "                                                # WHEN THERE IS NO OTHER DATA IN THAT ROW\n",
    "                                                # MIGHT NEED TO CHANGE TO 3\n",
    "\n",
    "    # Most of this is 0 values: \n",
    "    # game\n",
    "    # office\n",
    "    # weather \n",
    "    # Probably more accurate to fill it in with 0\n",
    "\n",
    "    # Rest - median \n",
    "\n",
    "    # s=df.isnull().cumsum()\n",
    "    # t1=df[(s==1).shift(-1).fillna(False)].stack().reset_index(level=0,drop=True)\n",
    "    # t2=df.lookup(s.idxmax()+1,s.idxmax().index)\n",
    "    # df.fillna(t1/(2**s)+t2*(1-0.5**s)*2/2)\n",
    "\n",
    "    #df.mood.fillna(method='bfill',inplace=True)  #for back fill - THIS IS CURRENTLY DONE IN THE DATA - This is good for MOOD\n",
    "    #df.mood.fillna(method='ffill',inplace=True)  #for forward-fill - THIS IS CURRENTLY DONE IN THE DATA - This is good for MOOD\n",
    "\n",
    "    #df.fillna({'game':0, 'office':0, 'weather': 0}, inplace=True) #fill in the empty rows \"call\" and \"sms\" with 0\n",
    "\n",
    "\n",
    "    #df.fillna(df.mood.mean(),inplace=True)       # use mean of the data to fill missind data - this not not good for MOOD (too much difference)\n",
    "\n",
    "    #df.mood.fillna(df.mood.median(),inplace=True)     # use median to till in missing data - this not not good for MOOD\n",
    "\n",
    "    #df = df.fillna(0) #fill in the empty rows with 0\n",
    "    #df.fillna({'game':0, 'office':0, 'weather': 0}, inplace=True) #fill in the empty rows \"call\" and \"sms\" with 0\n",
    "\n",
    "\n",
    "    #print (df)\n",
    "    \n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df['dayofweek'] = df['time'].dt.dayofweek\n",
    "    one_hot = pd.get_dummies(df['dayofweek'], prefix = 'day')\n",
    "    df = df.drop('dayofweek', axis=1)\n",
    "    df = df.join(one_hot)\n",
    "\n",
    "    df.to_csv(outpath, encoding='utf-8', index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data standardization \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpath = \"./data/table_5.csv\"\n",
    "outpath = \"./data/table_5_std.csv\"\n",
    "variables= ['mood', 'circumplex.arousal', 'circumplex.valence', 'activity', 'screen', 'call', 'sms', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game', 'appCat.office','appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather']\n",
    "df = pd.read_csv(inpath)\n",
    "df.describe()\n",
    "for variable in variables:\n",
    "    #print (df[variable].mean())\n",
    "    if variable in df.columns:\n",
    "        meanVal = df[variable].mean()\n",
    "        stdVal = df[variable].std()\n",
    "        df[variable] = (df[variable]-meanVal)/stdVal\n",
    "        #df.apply([variable]-[variable].mean())\n",
    "        #df.apply(lambda x: x-x.mean())\n",
    "df.describe()\n",
    "\n",
    "df.to_csv(outpath, encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
